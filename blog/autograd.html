<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Building an Autograd Engine: An Illustrative and Interactive Guide">
    <title>Building an Autograd Engine: An Illustrative and Interactive Guide</title>

    <!-- Inclue Style.css -->
    <link href='../static/css/style.css' rel='stylesheet' type='text/css'>

    <!-- Include the Lato Font CSS -->
    <link href='../static/css/font-lato.css' rel='stylesheet' type='text/css'>

    <!-- Include the MathJax JavaScript -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

    <!-- Include the D3.js JavaScript -->
    <script src="../static/js/d3.v7.min.js"></script>

    <!-- Include Prism.js CSS -->
    <link href="../static/css/prism.css" rel="stylesheet">
    <!-- Include Prism.js JavaScript -->
    <script src="../static/js/prism.js"></script>
</head>

<body>
  <div class="container-header">
    <header class="header-title">
        <div>
            <h2><a href="https://x0axz.com">x0axz</a></h2>
        </div>

        <div class="header-nav">
            <h4><a href="https://github.com/x0axz">Github</a></h4>
            <h4 class="header-nav-last"><a href="../about.html">About</a></h4>
        </div>
    </header>
    <hr>
  </div>

  <div class="container-article">
    <header>
      <h1>Building an Autograd Engine: An Illustrative and Interactive Guide</h1>
    </header>

      <p>
          In this article, we will build an Autograd engine and a neural network library that handle an N-dimensional array. Autograd is a tool used for derivative calculation. It tracks operations on values with enabled gradients and builds a dynamic computational graph â€” a graph without cycles. Input values serve as the leaves of the graph, while output values act as its roots. Gradients are computed by traversing the graph from root to leaf, applying the chain rule to multiply gradients at each step. <br /> <br />

          Neural networks are complex mathematical functions that are adjusted through a process called training to produce the desired output. Backpropagation is a key algorithm used in this training process. It computes gradients, which represent the change in loss for small adjustments in input weights and biases. These gradients are then utilised to update the weights and biases, with a learning rate applied to reduce the overall loss and train the neural network. This fine-tuning is also used to carefully adjust the parameters of a pre-trained model to adapt it to a specific task or dataset. The training process occurs iteratively, involving the calculation of multiple gradients. A computation graph is constructed to store these gradient functions. <br /> <br />

          Andrej Karaphy's <a href="https://github.com/karpathy/micrograd">Micrograd</a> and his <a href="https://youtu.be/VMj-3S1tku0">video tutorial</a> about building Micrograd, from which I would take a few examples, served as inspiration for this article. But this Autograd engine will accept N-dimensional array, whereas Microgard accepts scalar values only. <br /> <br />

          While assuming a basic understanding of Python programming, high school calculus, and neural networks, I'll provide various teaching methods for those who may not have that background. This includes line-by-line Python code explanations and visualizations of the output. The article includes an interactive area to explore derivatives, utilizing concepts from calculus. For a comprehensive understanding, I recommend watching 3Blue1Brown's <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">video series</a> on neural networks and backpropagation. In the <a href="https://youtu.be/tIeHLnjs5U8?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">final video</a>, he covers the Chain Rule, which is the mathematical foundation of backpropagation. Additionally, Jay Alammar's <a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks">article</a> on neural network basics is highly recommended. <br /> <br />

          Exploring the building blocks of neural networks and their training process, the journey begins with the basics of derivatives and covers various examples and methods. Delving into backpropagation, the focus is on understanding how to perform it manually and programmatically, including implementation techniques. Creating an autograd class from scratch and utilizing it to train a neural network on a dataset is the next step, leading to the development of a simple neural network library using our autograd class. <br /> <br />

          Let's dive right in and begin with the fundamentals.

          <h2>What is Derivates?</h2>

          Derivatives help us understand how things are changing at a specific point. They are like speedometers that measure
          the rate of change of a quantity or the slope of a curve. By calculating derivatives, we can gain insights into
          motion, growth, and other dynamic processes. <br /> <br />

          The derivative of a function is calculated using a specific mathematical formula. Let's denote the derivative of a function \( f(x) \) as \( f'(x) \) or \( \frac{dy}{dx} \), where \( y \) represents the dependent variable and \( x \) represents the independent variable. <br /> <br />

          The general formula for finding the derivative of a function \(f(x)\) is: <br />

          \( f'(x) = \lim_{h \to 0} \frac{{f(x + h) - f(x)}}{h} \) <br /> <br />

          In simpler terms, the derivative is calculated by taking the difference between the function values at two nearby
          points and dividing it by the difference in their corresponding x-values, as the difference approaches zero \( (h \to 0) \). <br /> <br />

          For example, if we have a function \( f(x) = 2x^2 \), we can find its derivative using the formula: <br />

          \( f'(x) = \lim_{{h \to 0}} \frac{{2(x + h)^2 - 2x^2}}{{h}} \) <br /> <br />

          By expanding and simplifying the expression, we can find the derivative \( f'(x) = 4x \). <br /> <br />

          This formula allows us to find the instantaneous rate of change or slope of the function at any specific point. It
          provides valuable information about how the function is changing at different locations along the x-axis. <br />

          <h2>Single-input derivative</h2>

          First, we'll create a basic function to compute the derivative of a scalar value. Subsequently, we'll perform theoretical differentiations on these expressions using identical values and verify that their outputs match. Lastly, we'll include an interactive section where we can adjust variable values and determine the derivatives of the given function with the chosen values. <br /> <br />

          <iframe src="../notebooks/autograd/derivative_one_input.html" width="100%" height="300px"></iframe> <br /> <br />

          Let's go through each line of the code and explain what it does:

          <ol>        
            <li>
              <code class="language-python">def f(x):</code> : This line defines a function named <code class="language-python">f</code> that takes a single input parameter <code class="language-python">x</code>. The function calculates and returns the result of the equation \( 3 * x^2 - 4 * x + 5 \). <br /> <br /> 
            </li>

            <li>
              <code class="language-python">return 3*x**2 - 4*x + 5</code> : This line specifies the equation that the function <code class="language-python">f</code> evaluates. It calculates the result of the quadratic expression \( 3 * x^2 - 4 * x + 5 \) and returns that value. <br /> <br />
            </li>

            <li>
              <code class="language-python">f(3.0)</code> : This line calls the function <code class="language-python">f</code> with an input value of <code class="language-python">3.0</code>. It calculates the result of the expression \( 3 * x^2 - 4 * x + 5 \) with \( x \) set to \( 3.0 \) and returns the result. <br /> <br />
            </li>

            <li>
              <code class="language-python">h = 0.001</code> : This line assigns a value of <code class="language-python">0.001</code> to the variable <code class="language-python">h</code>. In this context, <code class="language-python">h</code> represents a small value used to approximate the derivative of the function. <br /> <br />
            </li>

            <li>
              <code class="language-python">x = 3.0</code> : This line assigns a value of <code class="language-python">3.0</code> to the variable <code class="language-python">x</code>. This represents the point at which we want to evaluate the derivative of the function. <br /> <br />
            </li>

            <li>
              <code class="language-python">(f(x + h) - f(x))/h</code> : This line calculates the approximate derivative of the function <code class="language-python">def f(x):</code> at the point \( x \) using the finite difference method. It evaluates the expression \( \frac{{f(x + h) - f(x)}}{h} \), where \( f(x + h) \) represents the value of the function at \( x + h \), \( f(x) \) represents the value of the function at \( x \), and \( h \) is a small increment. This expression calculates the slope between two nearby points on the function and then divides it by \( h \) to approximate the derivative.
            </li>
          </ol>

          To summarize, the code defines a function <code class="language-python">f</code> that represents a quadratic equation. It then sets the values of <code class="language-python">h</code> and <code class="language-python">x</code>, representing the small increment and the point at which to evaluate the derivative, respectively. Finally, it calculates the approximate derivative of the function at <code class="language-python">x</code> using the finite difference method and returns the result.

          <h3>Theoretically differentiate the \( f(x) = 3x^2 - 4x + 5 \)</h3>

          To differentiate the function \( f(x) = 3x^2 - 4x + 5 \), we can apply the power rule and the constant rule of differentiation. The power rule states that the derivative of \( x^n \) (where \( n \) is a constant) is \( n * x^{(n-1)} \). The constant rule states that the derivative of a constant term is always zero. <br /> <br />

          Let's find the derivative of \( f(x) \) step by step: <br /> <br />

          \( f(x) = 3x^2 - 4x + 5 \) <br /> <br />

          To find the derivative, we differentiate each term separately: <br /> <br />

          The derivative of \( 3x^2 \) with respect to \( x \) is: <br />
          \( \frac{{d}}{{dx}} (3x^2) = 2 * 3x^{(2-1)} = 6x \) <br /><br />

          The derivative of \( -4x \) with respect to \( x \) is: <br />
          \( \frac{{d}}{{dx}} (-4x) = -4 \) <br /> <br />

          The derivative of the constant term \( 5 \) with respect to \( x \) is: <br />
          \( \frac{{d}}{{dx}} (5) = 0 \) <br /> <br />

          Rewrite the derivative as follows: <br /> <br />

          \( f'(x) = 6x - 4 \) <br /> <br />

          To find the value of the derivative at \( x = 3 \), we substitute \( x = 3 \) into the derivative expression: <br /> <br />

          \( f'(3) = 6 * 3 - 4 = 18 - 4 = 14 \) <br /> <br />

          Therefore, the derivative of \( f(x) = 3x^2 - 4x + 5 \) at \( x = 3 \) is \( 14 \).

          <h3>Theoretically differentiate the expression \(\frac{{f(x + h) - f(x)}}{h}\) where \( h = 0.001 \) and \( x = 3.0 \), and \( f(x) = 3x^2 - 4x + 5 \)</h3>

          To differentiate the expression \(\frac{{f(x + h) - f(x)}}{h}\) where \( h = 0.001 \) and \( x = 3.0 \), and \( f(x) = 3x^2 - 4x + 5 \), we can substitute these values into the expression and simplify it.  <br /> <br />

          Start by substituting the given values into the expression:  <br /> <br />

          \(\frac{{f(x + h) - f(x)}}{h}\) \( =  \) \(\frac{{f(3.0 + 0.001) - f(3.0)}}{{0.001}}\) <br /> <br />

          Calculate the values of \( f(3.0 + 0.001) \) and \( f(3.0) \): <br /> <br />

          \( f(3.0 + 0.001) = 3 * (3.0 + 0.001)^2 - 4 * (3.0 + 0.001) + 5 \) <br />
          \( = 3 * (3.001)^2 - 4 * (3.001) + 5 \) <br />
          \( = 3 * 9.006001 - 12.004 + 5 \) <br />
          \( = 27.018003 - 12.004 + 5 \) <br />
          \( = 20.014003 \) <br /> <br />

          \( f(3.0) = 3 * (3.0)^2 - 4 * (3.0) + 5 \) <br />
          \( = 3 * 9 - 12 + 5 \) <br />
          \( = 27 - 12 + 5 \) <br />
          \( = 20 \) <br /> <br />

          Substituting these values back into the expression: <br /> <br />

          \( \frac{{f(x + h) - f(x)}}{h} \) <br /> <br />
          \( = \frac{{20.014003 - 20}}{{0.001}} \) <br /> <br />
          \( = \frac{{0.014003}}{{0.001}} \) <br /> <br />
          \( = 14.003 \) <br /> <br />

          Therefore, the value of the expression \( \frac{{f(x + h) - f(x)}}{h} \) when \( h = 0.001 \) and \( x = 3.0 \) is \( 14.003 \).

          <div class="container-interactive-section">
              <h3 id="interactive-single-input-derivatives">Play with single-input derivative!</h3>
              <div class="single-input-derivative-slider-container">
                <label class="single-input-derivative-slider-label">h:</label>
                <input type="range" id="single-input-derivative-slider-h" min="-1" max="1" step="0.001" value="0.001">
                <span id="single-input-derivative-slider-h-value">0.001</span>
              </div>
              <div class="single-input-derivative-slider-container">
                <label class="single-input-derivative-slider-label">x:</label>
                <input type="range" id="single-input-derivative-slider-x" min="0" max="10" step="1" value="3">
                <span id="single-input-derivative-slider-x-value">3</span>
              </div>
              <div class="single-input-derivative-slider-container">
                <label class="single-input-derivative-slider-label">a:</label>
                <input type="range" id="single-input-derivative-slider-a" min="-10" max="10" step="0.1" value="3.0">
                <span id="single-input-derivative-slider-a-value">3.0</span>
              </div>
              <div class="single-input-derivative-slider-container">
                <label class="single-input-derivative-slider-label">b:</label>
                <input type="range" id="single-input-derivative-slider-b" min="-10" max="10" step="0.1" value="-4.0">
                <span id="single-input-derivative-slider-b-value">-4.0</span>
              </div>
              <div class="single-input-derivative-slider-container">
                <label class="single-input-derivative-slider-label">c:</label>
                <input type="range" id="single-input-derivative-slider-c" min="-10" max="10" step="0.1" value="5.0">
                <span id="single-input-derivative-slider-c-value">5.0</span>
              </div> <br />

              <div id="single-input-derivative-result">
                <div class="single-input-derivative-result-line-1">
                  Derivative of 
                  <span style="letter-spacing: 0.15em;">
                      <span id="single-input-derivative-result-line-1-a-value">3.0</span>
                      \( x^2 \) 
                      <span id="single-input-derivative-result-line-1-b-value">-4.0</span>
                      \( x \) 
                      <span id="single-input-derivative-result-line-1-c-value">+5.0</span>
                  </span> 
                  at \( x = \) 
                  <span id="single-input-derivative-result-line-1-x-value">3</span> 
                  is 
                  <span id="single-input-derivative-result-line-1-final-value" style="letter-spacing: 0.1em;"></span> 
                  <br /> 
                  and with adding slope of \( h = \) 
                  <span id="single-input-derivative-result-line-1-h-value" style="letter-spacing: 0.1em;">0.001</span> 
                  is 
                  <span id="single-input-derivative-result-line-1-final-value-with-h" style="letter-spacing: 0.1em;"></span> 
                </div> 
                <br />

                <div class="single-input-derivative-result-line-2">
                  Derivative of \( \frac{{f(x + h) - f(x)}}{h} \) where \( x = \) 
                  <span id="single-input-derivative-result-line-2-x-value">3</span> 
                  and \( h = \)
                  <span id="single-input-derivative-result-line-2-h-value">0.001</span>  
                  is
                  <span id="single-input-derivative-result-line-2-final-value-derivative" style="letter-spacing: 0.1em;"></span>
                  <br />
                  \( f(x + h) = \) 
                  <span style="letter-spacing: 0.1em;">
                      <span id="single-input-derivative-result-line-2-a-value-1">3.0</span>
                      \( (x + h)^2 \) 
                      <span id="single-input-derivative-result-line-2-b-value-1">-4.0</span>
                      \( (x + h) \) 
                      <span id="single-input-derivative-result-line-2-c-value-1">+5.0</span>
                    \( = \) 
                    <span id="single-input-derivative-result-line-2-final-value-f-x-plus-h" style="letter-spacing: 0.1em;"></span> 
                    </span> 
                    <br />
                  \( f(x) \) = 
                  <span style="letter-spacing: 0.15em;">
                      <span id="single-input-derivative-result-line-2-a-value-2">3.0</span>
                      \( x^2 \)
                      <span id="single-input-derivative-result-line-2-b-value-2">-4.0</span>
                      \( x \)
                      <span id="single-input-derivative-result-line-2-c-value-2">+5.0</span>
                  </span> 
                  \( = \)
                  <span id="single-input-derivative-result-line-2-final-value-f-x" style="letter-spacing: 0.1em;"></span> 
                </span>
                </div> 
              </br />
              </div>
          </div>

          <h2>Multi-input derivative</h2>

          In this section, we will create two functions. One function will involve incrementing one variable by a small amount proportional to the slope, allowing us to approximate the derivative of the expression with respect to that variable. We will then proceed to theoretically differentiate the given expression with respect to one of its variables using identical values, ensuring that the results match. Finally, an interactive section will be provided where variable values can be adjusted, and the variable for which the expression's derivative is computed can be changed. <br /> <br />

          <iframe src="../notebooks/autograd/derivative_multiple_input.html" width="100%" height="400px"></iframe>  <br /> <br />
          
          Let's go through each line of the code and explain what it does:

          <ol>
            <li>
              <code class="language-python">h = 0.0001</code> : This line assigns a value of <code class="language-python">0.0001</code> to the variable <code class="language-python">h</code>. In this code, <code class="language-python">h</code> is referred to as the "slope," but it is actually a small increment used for approximating the derivative. <br /> <br />
            </li>

            <li> 
              <code class="language-python">a = 2.0</code>, <code class="language-python">b = -3.0</code>, <code class="language-python">c = 10.0</code> : These lines assign specific values to the variables <code class="language-python">a</code>, <code class="language-python">b</code> and <code class="language-python">c</code>. These values represent the inputs for the function or expression we are working with. <br /> <br />
            </li>

            <li>
              <code class="language-python">d1 = a * b + c</code> : This line calculates the value of the expression \( a * b + c \) and assigns it to the variable <code class="language-python">d1</code>. <br /> <br />
            </li>

            <li>
              <code class="language-python">a += h</code> : This line increments the value of <code class="language-python">a</code> by adding <code class="language-python">h</code> to it. The purpose is to create a new value for a to be used in the next line. <br /> <br />
            </li>

            <li>
              <code class="language-python">d2 = a * b + c</code> : This line calculates the value of the expression \( a * b + c \) using the updated value of <code class="language-python">a</code> and assigns it to the variable <code class="language-python">d2</code>. <br /> <br />
            </li>

            <li>
              <code class="language-python">print('d1: ', d1)</code>, <code class="language-python">print('a: ', a)</code>, <code class="language-python">print('d2: ', d2)</code>, <code class="language-python">print('slope: ', (d2 - d1)/h)</code> : These lines print the values of <code class="language-python">d1</code>, <code class="language-python">a</code>, <code class="language-python">d2</code> and the slope (approximated derivative) calculated as \(\frac{{d2 - d1}}{h}\).
            </li>
          </ol>

          To summarize, the code sets up variables for the slope (<code class="language-python">h</code>) and the inputs (<code class="language-python">a</code>, <code class="language-python">b</code> and <code class="language-python">c</code>). It then performs calculations using these values to find <code class="language-python">d1</code> and <code class="language-python">d2</code>, representing the expressions \( a * b + c \) at different values of <code class="language-python">a</code>. Finally, it prints the values of <code class="language-python">d1</code>, <code class="language-python">a</code>, <code class="language-python">d2</code>, and the slope calculated using the finite difference method.

          <h3>Theoretically differentiate \( a * b + c \) with respect to \( a \)</h3>

          To differentiate the expression \( a * b + c \) with respect to \( a \), we need to find the derivative of the expression with respect to \( a \). Given that \( a = 2.0 \), \( b = -3.0 \), and \(c = 10.0 \), let's proceed with the differentiation. <br /> <br />

          The expression \( a * b + c \) involves multiplication and addition. To find the derivative with respect to \( a \), we differentiate each term separately: <br /> <br />

          The derivative of \( a * b \) with respect to \( a \) is: <br />
          \( \frac{{d}}{{da}} (a * b) = b \) <br /> <br />

          The derivative of \( c \) with respect to \( a \) is: <br />
          \( \frac{{d}}{{da}} (c) = 0 \) <br /> <br />

          Since \( c \) is a constant, its derivative with respect to \( a \) is always \( 0 \). <br /> <br />

          Rewrite the derivative as follows: <br /> <br />

          \( \frac{{d}}{{da}} (a * b + c) = b + 0 = b \) <br /> <br />

          Substituting the given values \( a = 2.0 \), \( b = -3.0 \), and \(c = 10.0 \) into the derivative expression, we get: <br /> <br />

          \( \frac{{d}}{{da}} (2.0 * -3.0 + 10.0) = -3.0 \) <br /> <br />

          Therefore, the derivative of the expression \( a * b + c \) with respect to \( a \), when \( a = 2.0 \), \( b = -3.0 \), and \(c = 10.0 \), is \( -3.0 \).

          <div class="container-interactive-section">
              <h3 id="interactive-multi-input-derivatives">Play with multi-input derivatives!</h3>
              <div class="multiple-input-derivative-slider-container">
                <label class="multiple-input-derivative-slider-label">h:</label>
                <input type="range" id="multiple-input-derivative-slider-h" min="-1" max="1" step="0.001" value="0.001">
                <span id="multiple-input-derivative-slider-h-value">0.001</span>
              </div>
              <div class="multiple-input-derivative-slider-container">
                <label class="multiple-input-derivative-slider-label">a:</label>
                <input type="range" id="multiple-input-derivative-slider-a" min="-10" max="10" step="0.1" value="2.0">
                <span id="multiple-input-derivative-slider-a-value">2.0</span>
              </div>
              <div class="multiple-input-derivative-slider-container">
                <label class="multiple-input-derivative-slider-label">b:</label>
                <input type="range" id="multiple-input-derivative-slider-b" min="-10" max="10" step="0.1" value="-3.0">
                <span id="multiple-input-derivative-slider-b-value">-3.0</span>
              </div>
              <div class="multiple-input-derivative-slider-container">
                <label class="multiple-input-derivative-slider-label">c:</label>
                <input type="range" id="multiple-input-derivative-slider-c" min="-10" max="10" step="0.1" value="10.0">
                <span id="multiple-input-derivative-slider-c-value">10.0</span>
              </div> 
              <br />
              <div>
                <label class="multiple-input-derivative-dropdown-label">Derivatives with respect to:</label>
                <select id="multiple-input-derivative-variable-selector">
                  <option value="a">a</option>
                  <option value="b">b</option>
                  <option value="c">c</option>
              </select>
              </div>
              <span>
                \( \text{D1} = a * b + c = \) 
                <span id="multiple-input-derivative-result-d1" style="letter-spacing: 0.1em;"></span>
              </span>
              <br />
              <span>
                <span id="multiple-input-derivative-selected-variable-1"></span> 
                += \( h = \) 
                <span id="multiple-input-derivative-result-selectedVariable-plus-h" style="letter-spacing: 0.1em;"></span>
              </span>
              <br />
              <span>
                \( \text{D2} = a * b + c = \) 
                <span id="multiple-input-derivative-result-d2" style="letter-spacing: 0.1em;"></span>
              </span> 
              <br />
              <span>
                \( \text{Slope} = \frac{{d2 - d1}}{{h}} = \) 
                <span id="multiple-input-derivative-result-slope" style="letter-spacing: 0.1em;"></span>
              </span> 
              <br /> <br />
              <span>
                Differentiate \( a * b + c \) with respect to 
                <span id="multiple-input-derivative-selected-variable-2"></span> 
                is 
                <span id="multiple-input-derivative-result-differentiate"></span> 
                <br />
                and with adding slope of \( h = \) 
                <span id="multiple-input-derivative-result-h-value" style="letter-spacing: 0.1em;">0.001</span> 
                is 
                <span id="multiple-input-derivative-result-differentiate-plus-h" style="letter-spacing: 0.1em;"></span>
            </span>
            <br /> <br />
          </div>

          <h2>Implementing backpropagation</h2>

          Now that a basic understanding of the derivative has been established, let's move forward with building the neural networks. The first step involves creating a class capable of handling N-dimensional array. The values and gradients will be stored in this class, as neural networks entail complex mathematical expressions. The development of these data structures will commence promptly. However, prior to that, the process of backpropagation will be manually performed. Subsequently, the automation of backpropagation will be achieved using the example-based explanation provided below.

          <h3>Manual backpropagation</h3>

          In this section, the exploration of the manual process of backpropagation will take place, which involves utilizing the chain rule to compute gradients. Backpropagation is a technique that leverages the chain rule to efficiently calculate gradients in a neural network with multiple layers. By propagating the error backwards through the layers, it becomes a crucial algorithm for training neural networks. <br /> <br />

          The chain rule plays a vital role by allowing us to break down the intricate task of adjusting weights and biases in a neural network into smaller, manageable steps. By considering the influence of each layer on the subsequent layer, we can ascertain how modifications in the network's parameters impact its output. This knowledge is then utilized to enhance the network's performance during the training process. <br /> <br />

          In order to comprehend the process, we will manually demonstrate the assignment and initialization of variables, followed by performing calculations on them to construct an expression. It is important to note that the subsequent section will illustrate how these tasks can be accomplished programmatically. <br /> <br />

          \( a = 2.0 \) <br />
          \( b = -3.0 \) <br />
          \( c = 10.0 \) <br />
          \( e = a * b \) <br />
          \( d = e + c \) <br />
          \( f = -2.0 \) <br />
          \( L = d * f \) <br /> <br />

          Substitute the expressions to obtain the expression for \( L \): <br />
          \( L = d * f \) <br />
          \( L = (e + c) * f \) <br />
          \( L = (a * b + c) * f \) <br /> <br />

          Expand the expression: <br />
          \( L = (2.0 * -3.0 + 10.0) * -2.0 \) <br />
          \( L = (-6.0 + 10.0) * -2.0 \) <br />
          \( L = 4.0 * -2.0 \) <br />
          \( L = -8.0 \) <br /> <br />

          Next, we will visually represent the output and observe its appearance. It is not necessary to concern ourselves with the specifics of graph generation at this point, as the upcoming section will provide detailed instructions on how to generate such graphs. <br /> <br />

          <img src="../static/diagrams/autograd/Manual Backpropagation.png" width="100%" alt="Manual Backpropagation Example" /> <br /> <br />

          Manually assign the gradient value of the variable \( L.grad \) to \( 1.0 \). <br /> <br />

          Upon updating the gradient value of the variable \( L.grad \) to \( 1.0 \), the resulting diagram will display the following visual representation. <br /> <br />

          <img src="../static/diagrams/autograd/L.grad.png" width="100%" alt="Update L.grad value" />

          <h4 id="differentiation-process-for-product-expressions">Differentiation process for product expressions</h4>

          Moving forward with the backpropagation process, our focus will now shift towards examining the derivatives of \( L \) with respect to \( d \) and \( f \). Our first step will involve calculating the derivative of \( d \). <br /> <br />

          To differentiate \( L \) with respect to \( d \), we treat all other variables (\( a \), \( b \), \( c \), and \( f \)) as constants since they don't depend on \( d \). The derivative of \( a \) constant multiplied by \( a \) variable is simply the constant itself. Therefore, the derivative of \( d \) with respect to \( d \) is \( 1 \): <br /> <br />

          \( \frac{{dL}}{{dd}} = f * 1 \) <br /> <br />
          \( \frac{{dL}}{{dd}} = -2.0 * 1 \) <br /> <br />
          \( \frac{{dL}}{{dd}} = -2.0 \) <br /> <br />

          So, the derivative of \( L \) with respect to \( d \), denoted as \( \frac{{dL}}{{dd}} \), is equal to \( -2.0 \). <br /> <br />

          We can prove that the derivative of \( L \) with respect to \( d \), denoted as \( \frac{{dL}}{{dd}} \), is equal to \( -2.0 \) using the definition of the derivative. <br /> <br />

          Given: <br />
          \( L = d * f \) <br /> <br />

          The derivative of \( L \) with respect to \( d \), denoted as \( \frac{{dL}}{{dd}} \), is defined as the limit of the difference quotient as \( h \) approaches \( 0 \): <br /> <br />

          \( \frac{{dL}}{{dd}} = \lim_{{h \to 0}} \frac{{L(d + h) - L(d)}}{{h}} \) <br /> <br />

          Substitute the expression for \( L \): <br /> <br />

          \( \frac{{dL}}{{dd}} = \lim_{{h \to 0}} \frac{{(d + h) * f - d * f}}{{h}} \) <br /> <br />

          Expand the expression: <br /> <br />

          \( \frac{{dL}}{{dd}} = \lim_{{h \to 0}} \frac{{d * f + h * f - d * f}}{{h}} \) <br /> <br />

          Simplify: <br /> <br />

          \( \frac{{dL}}{{dd}} = \lim_{{h \to 0}} \frac{{h \cdot f}}{{h}} \) <br /> <br />

          Cancel out \( h \): <br /> <br />

          \( \frac{{dL}}{{dd}} = \lim_{{h \to 0}} f \) <br /> <br />

          Substitute the value of \( f \): <br /> <br />

          \( \frac{{dL}}{{dd}} = -2.0 \) <br /> <br />

          Therefore, the derivative of \( L \) with respect to \( d \), denoted as \( \frac{{dL}}{{dd}} \), is indeed equal to \( -2.0 \). <br /> <br />

          We can apply the same mathematical principles to solve the derivatives of the product expressions. For instance, the derivative of \( L \) with respect to \( d \) is equal to \( f \), and the derivative of \( L \) with respect to \( f \) is equal to \( d \), with a specific value of \( 4.0 \). By following this logic, you can solve similar examples without the need for repetitive differentiation. <br /> <br />

          Update the gradient values of the variables \( d \) and \( f \). Specifically, \( d.grad \) will be assigned a value of \( -2.0 \), and \( f.grad \) will be assigned a value of \( 4.0 \). Once these updates are applied, the resulting diagram will display the following visual representation. <br /> <br />

          <img src="../static/diagrams/autograd/d.grad and f.grad.png" width="100%" alt="Update d.grad and f.grad values" />

          <h4 id="differentiation-process-for-sum-expressions">Differentiation process for sum expressions</h4>

          The next step involves determining the gradient values of the variables \( e \) and \( c \). <br /> <br />

          To find the derivative of the expression \( d = c + e \) with respect to \( c \), we differentiate the expression with respect to \( c \) while treating \( e \) as a constant. Since the derivative of a constant with respect to any variable is \( 0 \), the derivative of \( e \) with respect to \( c \) is \( 0 \). <br /> <br />

          Therefore, the derivative of \( d \) with respect to \( c \), written as \( \frac{{dd}}{{dc}} \), is simply \( 1 \). <br /> <br />

          To prove the derivative of \( d = c + e \) with respect to \( c \) using the definition of the derivative, we need to compute the following limit: <br /> <br />

          \( \frac{{dd}}{{dc}} = \lim_{{h \to 0}} \frac{{d(c + h) - d(c)}}{{h}} \) <br /> <br />

          Start by evaluating \( d(c + h) \) and \( d(c) \): <br /> <br />

          \( d(c + h) = (c + h) + e = c + h + e \) <br />
          \( d(c) = c + e \) <br /> <br />

          Substituting these values into the limit expression, we have: <br /> <br />

          \( \frac{{dd}}{{dc}} = \lim_{{h \to 0}} \frac{{c + h + e - (c + e)}}{{h}} \) <br /> <br />

          Simplifying the expression inside the limit: <br /> <br />

          \( \frac{{dd}}{{dc}} = \lim_{{h \to 0}} \frac{{c + h + e - c - e}}{{h}} \) <br /> <br />
          \( \lim_{{h \to 0}} \frac{{h}}{{h}} \) <br /> <br />
          \( \lim_{{h \to 0}} 1 \) <br /> <br />
          \( = 1 \) <br /> <br />

          Therefore, we have shown that the derivative of \( d = c + e \) with respect to \( c \) is \( 1 \), which confirms our initial result. <br /> <br />

          We can apply the same mathematical principles to solve the derivatives of sum expressions. For instance, the derivative of \( d \) with respect to \( c \) is equal to \( 1.0 \), and the derivative of \( d \) with respect to \( e \) is equal to \( 1.0 \). By following this logic, you can solve similar examples without the need for repetitive differentiation. <br /> <br />

          We have obtained the local derivatives of \( \frac{{dL}}{{dd}} \), \( \frac{{dd}}{{dc}} \), and \( \frac{{dd}}{{de}} \). Now, let's determine the derivatives of \( L \) with respect to \( c \) and \( e \), represented as \( \frac{{dL}}{{dc}} \) and \( \frac{{dL}}{{de}} \), respectively. <br /> <br />

          To find \( \frac{{dL}}{{dc}} \), apply the Chain rule: <br /> <br />
          \( \frac{{dL}}{{dc}} = \frac{{dL}}{{dd}} * \frac{{dd}}{{dc}} \) <br /> <br />
          \( \frac{{dL}}{{dc}} = -2.0 * 1.0 \) <br /> <br />
          \( \frac{{dL}}{{dc}} = -2.0 \) <br /> <br />

          Similarly, employing the Chain rule, we can find \( \frac{{dL}}{{de}} \): <br /> <br />
          \( \frac{{dL}}{{de}} = \frac{{dL}}{{dd}} * \frac{{dd}}{{de}} \) <br /> <br />
          \( \frac{{dL}}{{de}} = -2.0 * 1.0 \) <br /> <br />
          \( \frac{{dL}}{{de}} = -2.0 \) <br /> <br />

          Update the gradient values of the variables \( c \) and \( e \). Specifically, \( c.grad \) will be assigned a value of \( -2.0 \), and \( e.grad \) will be assigned a value of \( -2.0 \). Once these updates are applied, the resulting diagram will display the following visual representation. <br /> <br />

          <img src="../static/diagrams/autograd/c.grad and e.grad.png" width="100%" alt="Update c.grad and e.grad values" /> <br /> <br />

          Finally, in order to determine the local derivative of \( e \) with respect to \( a \) and \( b \), we will employ the <a href="#differentiation-process-for-product-expressions">differentiation method for product expressions</a>. According to this approach, the derivative of \( e \) with respect to \( a \) is equivalent to \( b \), whose value is \( -3.0 \), and the derivative of \( e \) with respect to \( b \) is equal to \( a \), whose value is \( 2.0 \). Therefore, we now possess the values of \( \frac{{dL}}{{de}} \), \( \frac{{de}}{{da}} \), and \( \frac{{de}}{{db}} \). To find the values of \( \frac{{dL}}{{da}} \) and \( \frac{{dL}}{{db}} \), we will utilize the chain rule, as demonstrated above. <br /> <br />

          To find \( \frac{{dL}}{{da}} \), apply the Chain rule: <br /> <br />
          \( \frac{{dL}}{{da}} = \frac{{dL}}{{de}} * \frac{{de}}{{db}} \) <br /> <br />
          \( \frac{{dL}}{{da}} = -2.0 * -3.0 \) <br /> <br />
          \( \frac{{dL}}{{da}} = 6.0 \) <br /> <br />

          Similarly, employing the Chain rule, we can find \( \frac{{dL}}{{db}} \): <br /> <br />
          \( \frac{{dL}}{{db}} = \frac{{dL}}{{de}} * \frac{{de}}{{da}} \) <br /> <br />
          \( \frac{{dL}}{{db}} = -2.0 * 2.0 \) <br /> <br />
          \( \frac{{dL}}{{db}} = -4.0 \) <br /> <br />

          Update the gradient values of the variables \( a \) and \( b \). Specifically, \( a.grad \) will be assigned a value of \( 6.0 \), and \( b.grad \) will be assigned a value of \( -4.0 \). Once these updates are applied, the resulting diagram will display the following visual representation. <br /> <br />

          <img src="../static/diagrams/autograd/a.grad and b.grad.png" width="100%" alt="Update a.grad and b.grad values" /> <br /> <br />

          Now that we have gained an understanding of manual backpropagation, if you wish to verify the accuracy of the gradient values obtained through differentiation, you can explore the interactive sections for <a href="#interactive-single-input-derivatives">single</a> and <a href="#interactive-multi-input-derivatives">multiple</a> input derivatives. Additionally, we will now proceed to automate the backpropagation process. By comparing the gradient values obtained through automation with the manually calculated values, we can ensure the accuracy of our process and calculations.

          <h3>Automate backpropagation</h3>

          The functionality of the <code class="language-python">Value</code> class is to implement automatic differentiation, which allows for computing gradients of mathematical expressions with respect to their inputs. The class represents a value or variable in a computational graph, where each value can be connected to other values through mathematical operations. <br /> <br />

          Let's proceed with creating one function at a time within the <code class="language-python">Value</code> class, and meticulously analyze each line of the code to provide an explanation of its purpose and functionality. <br /> <br />

          <script src="https://gist.github.com/x0axz/1962001ec7214b3790989ae7552b333c.js"></script>

          The <code class="language-python">__init__</code> method initializes an instance of the <code class="language-python">Value</code> class. It takes in a <code class="language-python">data</code> parameter, which can be a NumPy array or any data that can be converted into a NumPy array. If <code class="language-python">data</code> is already a NumPy array, it is stored directly. Otherwise, it is converted into a NumPy array using <code class="language-python">np.array(data)</code>. <br /> <br />

          The <code class="language-python">grad</code> attribute is initialized as an array of zeros with the same shape as the <code class="language-python">data</code> array. This attribute is used to store the gradient (derivative) of the node with respect to some output. <br /> <br />

          The <code class="language-python">_backward</code> attribute is a placeholder for the backward function, which is used for backpropagation in neural networks. It is initially set to a lambda function that does nothing. <br /> <br />

          The <code class="language-python">_prev</code> attribute is a set that keeps track of the previous nodes (parents) in the computation graph. This is used to determine the dependencies between nodes during backpropagation. <br /> <br />

          The <code class="language-python">_op</code> attribute stores the operation associated with the current node. This can be used to identify the type of operation performed on the node, such as addition, multiplication, etc. <br /> <br />

          The <code class="language-python">label</code> attribute is an optional label for the node, which can be used for identification or debugging purposes. <br /> <br />

          In summary, this code defines a class that represents a node in a computation graph, with functionality to store data, compute gradients, and manage dependencies. <br /> <br />

          <script src="https://gist.github.com/x0axz/d588698465785b2b8d3ec4585b23dc6f.js"></script>

          The <code class="language-python">__add__</code> method is overridden in the <code class="language-python">Value</code> class to define the behavior of the addition operation (<code class="language-python">+</code>) between two <code class="language-python">Value</code> objects. This method allows adding two <code class="language-python">Value</code> instances together. <br /> <br />

          Inside the <code class="language-python">__add__</code> method, the <code class="language-python">other</code> parameter is checked to see if it is already a <code class="language-python">Value</code> object. If not, it is converted into a <code class="language-python">Value</code> object using <code class="language-python">Value(other).</code> <br /> <br />

          A new <code class="language-python">Value</code> instance called <code class="language-python">out</code> is created to represent the result of the addition operation. The data of <code class="language-python">out</code> is obtained by adding the data of <code class="language-python">self</code> (the current instance) and the data of <code class="language-python">other</code>. The parents of <code class="language-python">out</code> are set as <code class="language-python">self</code> and <code class="language-python">other</code>, and the operation associated with <code class="language-python">out</code> is set as '<code class="language-python">+</code>'. <br /> <br />

          A nested function <code class="language-python">_backward</code> is defined within the <code class="language-python">__add__</code> method. This function is responsible for calculating the gradients using the chain rule and updating the gradients of the operands (<code class="language-python">self</code> and <code class="language-python">other</code>). The gradient of <code class="language-python">self</code> is updated by adding the element-wise multiplication of \( 1.0 \) and the gradient of <code class="language-python">out</code>. Similarly, the gradient of <code class="language-python">other</code> is updated by adding the element-wise multiplication of \( 1.0 \) and the gradient of <code class="language-python">out</code>. <br /> <br />

          The <code class="language-python">_backward</code> function is then assigned to the <code class="language-python">_backward</code> attribute of the out instance, effectively replacing the placeholder backward function with the actual backward function. <br /> <br />

          Finally, the <code class="language-python">out</code> instance representing the result of the addition operation is returned. <br /> <br />

          In summary, the <code class="language-python">__add__</code> method allows adding two <code class="language-python">Value</code> instances together and defines the necessary computations for calculating gradients during backpropagation. <br /> <br />

          <script src="https://gist.github.com/x0axz/ac65eb07cd62a8cccc4fd29a84d1956a.js"></script>

          The <code class="language-python">__mul__</code> method is overridden in the <code class="language-python">Value</code> class to define the behavior of the multiplication operation (<code class="language-python">*</code>) between two <code class="language-python">Value</code> objects. This method allows multiplying two <code class="language-python">Value</code> instances together. <br /> <br />

          Similar to the <code class="language-python">__add__</code> method, the <code class="language-python">other</code> parameter is checked to determine if it is already a <code class="language-python">Value</code> object. If not, it is converted into a <code class="language-python">Value</code> object using <code class="language-python">Value(other)</code>. <br /> <br />

          A new <code class="language-python">Valu</code>e instance called <code class="language-python">out</code> is created to represent the result of the multiplication operation. The data of <code class="language-python">out</code> is obtained by element-wise multiplying the data of <code class="language-python">self</code> (the current instance) and the data of <code class="language-python">other</code>. The parents of <code class="language-python">out</code> are set as <code class="language-python">self</code> and <code class="language-python">other</code>, and the operation associated with <code class="language-python">out</code> is set as '<code class="language-python">*</code>'. <br /> <br />

          A nested function <code class="language-python">_backward</code> is defined within the <code class="language-python">__mul__</code> method. This function calculates the gradients using the chain rule and updates the gradients of the operands (<code class="language-python">self</code> and <code class="language-python">other</code>). The gradient of self is updated by adding the element-wise multiplication of <code class="language-python">other.data</code> and the gradient of out. Similarly, the gradient of <code class="language-python">other</code> is updated by adding the element-wise multiplication of <code class="language-python">self.data</code> and the gradient of <code class="language-python">out</code>. <br /> <br />

          The <code class="language-python">_backward</code> function is assigned to the <code class="language-python">_backward</code> attribute of the <code class="language-python">out</code> instance, replacing the placeholder backward function. <br /> <br />

          Finally, the <code class="language-python">out</code> instance representing the result of the multiplication operation is returned. <br /> <br />

          In summary, the <code class="language-python">__mul__</code> method allows multiplying two <code class="language-python">Value</code> instances together and defines the necessary computations for calculating gradients during backpropagation. <br /> <br />

          <script src="https://gist.github.com/x0axz/92f9a8bfd6b9772d2b61a9a217740d95.js"></script>

          The <code class="language-python">__pow__</code> method is overridden in the <code class="language-python">Value</code> class to define the behavior of the power operation (<code class="language-python">**</code>) between two <code class="language-python">Value</code> objects. This method allows raising a <code class="language-python">Value</code> instance to a scalar exponent or element-wise raising a <code class="language-python">Value</code> instance to another <code class="language-python">Value</code> instance. <br /> <br />

          If the <code class="language-python">other</code> parameter is a scalar (integer or float), an element-wise power operation is performed. The <code class="language-python">self.data</code> is raised to the power of <code class="language-python">other</code>, and the result is stored in <code class="language-python">out_data</code>. A new <code class="language-python">Value</code> instance called <code class="language-python">out</code> is created with <code class="language-python">out_data</code> as the data, <code class="language-python">self</code> as the parent, and <code class="language-python">f'**{other}'</code> as the operation label. <br /> <br />

          The <code class="language-python">_backward</code> function is defined for the scalar exponent case. It calculates the gradients using the chain rule and updates the gradient of <code class="language-python">self</code>. The gradient update involves multiplying <code class="language-python">other</code> with <code class="language-python">np.power(self.data, other - 1)</code> and then multiplying the result with the gradient of <code class="language-python">out</code>. The computed gradient is added to <code class="language-python">self.grad</code>. <br /> <br />

          If the <code class="language-python">other</code> parameter is a <code class="language-python">Value</code> instance, an element-wise power operation is performed. The <code class="language-python">self.data</code> is raised to the power of <code class="language-python">other.data</code>, and the result is stored in <code class="language-python">out_data</code>. A new <code class="language-python">Value</code> instance called <code class="language-python">out</code> is created with <code class="language-python">out_data</code> as the data, <code class="language-python">self</code> and <code class="language-python">other</code> as parents, and '<code class="language-python">**</code>' as the operation label. <br /> <br />

          The <code class="language-python">_backward</code> function is defined for the <code class="language-python">Value</code> exponent case. It calculates the gradients using the chain rule and updates the gradients of <code class="language-python">self</code> and <code class="language-python">other</code>. The gradient update for <code class="language-python">self</code> involves multiplying <code class="language-python">other.data</code> with <code class="language-python">np.power(self.data, other.data - 1)</code> and then multiplying the result with the gradient of <code class="language-python">out</code>. The computed gradient is added to <code class="language-python">self.grad</code>. The gradient update for <code class="language-python">other</code> involves multiplying <code class="language-python">np.log(self.data)</code> with the gradient of <code class="language-python">out</code> and adding it to <code class="language-python">other.grad</code>. <br /> <br />

          In case the <code class="language-python">other</code> parameter is neither a scalar nor a <code class="language-python">Value</code> instance, a <code class="language-python">TypeError</code> is raised to indicate unsupported operand types. <br /> <br />

          In summary, the <code class="language-python">__pow__</code> method allows raising a <code class="language-python">Value</code> instance to a scalar exponent or element-wise raising a <code class="language-python">Value</code> instance to another <code class="language-python">Value</code> instance. It defines the necessary computations for calculating gradients during backpropagation in each case. <br /> <br />

          <script src="https://gist.github.com/x0axz/0e534a8626b247ebcac57cbed17ebcd1.js"></script>

          The code provides additional method overrides in the <code class="language-python">Value</code> class: <br />

          <ul>
            <li>
              <code class="language-python">__radd__</code> method allows performing right addition by the <code class="language-python">Value</code> instance. It returns the result of addition <code class="language-python">self</code> by <code class="language-python">other</code> using the <code class="language-python">+</code> operator. <br /> <br />
            </li>

            <li>
              <code class="language-python">__rmul__</code> method allows performing right multiplication by the <code class="language-python">Value</code> instance. It returns the result of multiplying <code class="language-python">self</code> by <code class="language-python">other</code> using the <code class="language-python">*</code> operator. <br /> <br />
            </li>

            <li>
              <code class="language-python">__truediv__</code> method allows performing true division by the <code class="language-python">Value</code> instance. It returns the result of dividing <code class="language-python">self</code> by <code class="language-python">other</code> using the <code class="language-python">/</code> operator. This is achieved by multiplying <code class="language-python">self</code> by <code class="language-python">other</code> raised to the power of \( -1 \). <br /> <br />
            </li>

            <li>
              <code class="language-python">__neg__</code> method allows performing negation of the <code class="language-python">Value</code> instance. It returns the negation of <code class="language-python">self</code> by multiplying it with \( -1 \). <br /> <br />
            </li>

            <li>
              <code class="language-python">__sub__</code> method allows performing subtraction of a <code class="language-python">Value</code> instance. It returns the result of subtracting <code class="language-python">other</code> from <code class="language-python">self</code> using the <code class="language-python">-</code> operator. This is achieved by adding <code class="language-python">self</code> to the negation of <code class="language-python">other</code>. <br />
            </li>
          </ul>

          These method overrides provide convenient shorthand notations for arithmetic operations involving <code class="language-python">Value</code> instances and allow for a more expressive and intuitive usage of the <code class="language-python">Value</code> class. <br /> <br />

          <script src="https://gist.github.com/x0axz/15f44b32bae67a4071c7e9ca8e25bf86.js"></script>

          The <code class="language-python">exp</code> method is defined in the <code class="language-python">Value</code> class to compute the element-wise exponential of a <code class="language-python">Value</code> instance. <br /> <br />

          The method begins by retrieving the <code class="language-python">self.data</code> array and assigning it to the variable <code class="language-python">x</code>. <br /> <br />

          A new <code class="language-python">Value</code> instance called <code class="language-python">out</code> is created with the exponential of <code class="language-python">x</code> as the data. The parents of <code class="language-python">out</code> are set to <code class="language-python">(self,)</code>, indicating that <code class="language-python">self</code> is the parent node. The operation label is set as <code class="language-python">'exp'</code> to represent the exponential operation. <br /> <br />

          A nested function <code class="language-python">_backward</code> is defined to calculate the gradients using the chain rule. In this case, the gradient of <code class="language-python">self</code> is updated by adding the element-wise multiplication of <code class="language-python">out.data</code> and the gradient of <code class="language-python">out</code>. <br /> <br />

          The <code class="language-python">_backward</code> function is assigned to the <code class="language-python">_backward</code> attribute of the <code class="language-python">out</code> instance, replacing the placeholder backward function. <br /> <br />

          Finally, the <code class="language-python">out</code> instance representing the result of the exponential operation is returned. <br /> <br />

          In summary, the <code class="language-python">exp</code> method computes the element-wise exponential of a <code class="language-python">Value</code> instance and defines the necessary computations for calculating gradients during backpropagation. <br /> <br />

          <script src="https://gist.github.com/x0axz/dc381775b1f886739fc75b60a3243f3b.js"></script>

          The <code class="language-python">tanh</code> method is defined in the <code class="language-python">Value</code> class to compute the element-wise hyperbolic tangent of a <code class="language-python">Value</code> instance. <br /> <br />

          The method begins by retrieving the <code class="language-python">self.data</code> array and assigning it to the variable <code class="language-python">x</code>. <br /> <br />

          The hyperbolic tangent of <code class="language-python">x</code> is calculated using <code class="language-python">np.tanh(x)</code> and assigned to the variable <code class="language-python">t</code>. <br /> <br />

          A new <code class="language-python">Value</code> instance called <code class="language-python">out</code> is created with <code class="language-python">t</code> as the data. The parent of <code class="language-python">out</code> is set to <code class="language-python">(self,)</code>, indicating that <code class="language-python">self</code> is the parent node. The operation label is set as <code class="language-python">'tanh'</code> to represent the hyperbolic tangent operation. <br /> <br />

          A nested function <code class="language-python">_backward</code> is defined to calculate the gradients using the chain rule. In this case, the gradient of <code class="language-python">self</code> is updated by adding the element-wise multiplication of <code class="language-python">(1 - t**2)</code> and the gradient of <code class="language-python">out</code>. The derivative <code class="language-python">(1 - t**2)</code> corresponds to the derivative of the hyperbolic tangent function. <br /> <br />

          The <code class="language-python">_backward</code> function is assigned to the <code class="language-python">_backward</code> attribute of the <code class="language-python">out</code> instance, replacing the placeholder backward function. <br /> <br />

          Finally, the <code class="language-python">out</code> instance representing the result of the hyperbolic tangent operation is returned. <br /> <br />

          In summary, the <code class="language-python">tanh</code> method computes the element-wise hyperbolic tangent of a <code class="language-python">Value</code> instance and defines the necessary computations for calculating gradients during backpropagation. <br /> <br />

          <script src="https://gist.github.com/x0axz/1aa4402c1fd791344fade8baa4e97439.js"></script>

          The <code class="language-python">backward</code> method is defined in the <code class="language-python">Value</code> class to perform backpropagation and compute gradients for the computational graph. <br /> <br />

          The method begins by initializing an empty list called <code class="language-python">topo</code> to store the topological order of nodes and a set called <code class="language-python">visited</code> to keep track of visited nodes. <br /> <br />

          A nested function called <code class="language-python">build_topo</code> is defined to recursively build the topological order of nodes starting from a given node <code class="language-python">v</code>. The function checks if the node <code class="language-python">v</code> has been visited before. If not, it adds <code class="language-python">v</code> to the <code class="language-python">visited</code> set and recursively calls <code class="language-python">build_topo</code> for each child node of <code class="language-python">v</code>. After visiting all child nodes, it appends <code class="language-python">v</code> to the <code class="language-python">topo</code> list. This ensures that nodes are added to the <code class="language-python">topo</code> list in a topological order, i.e., parents before children. <br /> <br />

          The <code class="language-python">build_topo</code> function is invoked with <code class="language-python">self</code> as the starting node to build the topological order of nodes. <br /> <br />

          The gradient of the output node (assumed to be a scalar loss) is set to ones using <code class="language-python">np.ones_like(self.data)</code>. This initializes the gradient for backpropagation. <br /> <br />

          The method then iterates through the nodes in reverse order using the <code class="language-python">reversed</code> function on the <code class="language-python">topo</code> list. For each node, it calls the <code class="language-python">_backward</code> function associated with that node. The <code class="language-python">_backward</code> function was assigned in the respective arithmetic and mathematical operation methods, and it calculates and updates the gradients of the node's parents based on the chain rule. <br /> <br />

          By traversing the nodes in reverse topological order, the <code class="language-python">backward</code> method ensures that the gradients are calculated correctly and efficiently. <br /> <br />

          In summary, the <code class="language-python">backward</code> method performs backpropagation by traversing the computational graph in reverse topological order and calling the <code class="language-python">_backward</code> function for each node to compute the gradients. <br /> <br />

          <script src="https://gist.github.com/x0axz/dd887cc7855fc4d0054192acb5b7224f.js"></script>

          The <code class="language-python">__repr__</code> method is overridden in the <code class="language-python">Value</code> class to provide a string representation of a <code class="language-python">Value</code> instance. <br /> <br />

          The method returns a string that includes the value of the <code class="language-python">data</code> attribute of the <code class="language-python">Value</code> instance. It uses f-string formatting to construct the string representation in the format <code class="language-python">Value(data=<data>)</code>, where <code class="language-python"><data></code> is replaced with the actual value of <code class="language-python">self.data</code>. <br /> <br />

          This allows for a concise and informative representation of a <code class="language-python">Value</code> instance when it is printed or converted to a string. <br /> <br />

          In summary, the <code class="language-python">__repr__</code> method provides a string representation of a <code class="language-python">Value</code> instance, making it easier to understand and debug the object when printed or converted to a string.

          <h3>Visualize the expressions</h3>

          Since the <code class="language-python">Value</code> class performs computation on expressions that can potentially be large, depending on the inputs, it would be beneficial to have a visually appealing representation of these expressions. This visualization would allow us to better understand and gain a sense of how the results of these expressions look, facilitating comprehension and analysis. <br /> <br />

          To visualize the computational graph, we will use the <a href="https://graphviz.org">Graphviz</a> library. <br /> <br />

          We will create a pair of functions. The initial function will trace the computational graph starting from a given node and provide a collection of nodes and edges that depict the graph. The second function will accept a root node as input and produce a visual representation of the computational graph rooted at that node. Furthermore, we will meticulously examine each line of the code to ensure precise explanations of its purpose and functionality. <br /> <br />

          <script src="https://gist.github.com/x0axz/680a0e9e1aebc5345ccb1071604a6977.js"></script>

          The code defines a function called <code class="language-python">trace</code> that takes a <code class="language-python">root</code> node as input and returns a set of nodes and edges that represent a trace of the computational graph rooted at the <code class="language-python">root</code> node. <br /> <br />

          The function first initializes empty sets called <code class="language-python">nodes</code> and <code class="language-python">edges</code> to store the nodes and edges of the graph, respectively. <br /> <br />

          A nested function called <code class="language-python">build</code> is defined to recursively build the set of nodes and edges starting from a given node <code class="language-python">v</code>. The function checks if the node <code class="language-python">v</code> is already in the <code class="language-python">nodes</code> set. If not, it adds <code class="language-python">v</code> to the <code class="language-python">nodes</code> set and proceeds to iterate over the previous nodes (parents) of <code class="language-python">v</code>. For each previous node (<code class="language-python">child</code>), it adds an edge from <code class="language-python">child</code> to <code class="language-python">v</code> in the <code class="language-python">edges</code> set and recursively calls <code class="language-python">build</code> on the previous node (<code class="language-python">child</code>). <br /> <br />

          The <code class="language-python">build</code> function is invoked initially with the <code class="language-python">root</code> node to start building the set of nodes and edges. <br /> <br />

          Finally, the function returns the set of nodes (<code class="language-python">nodes</code>) and edges (<code class="language-python">edges</code>). <br /> <br />

          In summary, the <code class="language-python">trace</code> function traces the computational graph rooted at a given node and returns the set of nodes and edges that represent the graph. <br /> <br />

          <script src="https://gist.github.com/x0axz/f7762bc6d395e5b0fa754df2d3ae4261.js"></script>

          The code defines a function called <code class="language-python">draw_dot</code> that takes a <code class="language-python">root</code> node as input and generates a visualization of the computational graph rooted at the <code class="language-python">root</code> node using the Graphviz library. <br /> <br />

          The function begins by creating a <code class="language-python">Digraph</code> object named <code class="language-python">dot</code> with the format set to SVG and the graph attribute <code class="language-python">rankdir</code> set to 'LR' to arrange the nodes from left to right. <br /> <br />

          The <code class="language-python">trace</code> function is called to retrieve the set of nodes and edges that represent the computational graph rooted at the <code class="language-python">root</code> node. <br /> <br />

          The function then iterates over the nodes in the <code class="language-python">nodes</code> set. For each node n, it assigns a unique ID (<code class="language-python">uid</code>) based on the node's ID using <code class="language-python">str(id(n))</code>. It converts the <code class="language-python">data</code> and <code class="language-python">grad</code> arrays of the node into string representations using <code class="language-python">np.array2string</code> with specified precision, separator, and suppression of small values. It creates the label for the node by combining the node's label, data string, and gradient string. The node is added to the graph with the unique ID, label, and shape 'record' to visualize the node as a record-shaped box. <br /> <br />

          If the node has an associated operation (<code class="language-python">n._op</code>), it adds an additional node to the graph with a unique ID based on the node's ID and operation, and labels it with the operation. An edge is added from the operation node to the current node (<code class="language-python">n</code>) in the graph. <br /> <br />

          Next, the function iterates over the <code class="language-python">edges</code> in the edges set. For each edge (<code class="language-python">n1</code>, <code class="language-python">n2</code>), it adds an edge from the ID of the first node (<code class="language-python">n1</code>) to the ID of the second node (<code class="language-python">n2</code>) concatenated with the operation name (<code class="language-python">n2._op</code>). <br /> <br />

          Finally, the function returns the generated graph (<code class="language-python">dot</code>). <br /> <br />

          Overall, the <code class="language-python">draw_dot</code> function generates a visualization of the computational graph rooted at a given node by creating nodes and edges in a <code class="language-python">Digraph</code> object using Graphviz. The nodes represent the <code class="language-python">Value</code> instances in the graph, and the edges represent the dependencies between the nodes based on their parent-child relationships. The labels of the nodes display information such as the node's label, data, and gradient.

          <h3>Example 1: Compute a forward pass on 2D array and visualize the computation graph</h3>

          In this first example, we will perform a forward pass on the tanh function and subsequently visualize it. The inputs, weights, and biases will all be 2D array with the same values to ensure accurate calculations. You can copy the code and modify these values to observe different results. <br /> <br />

          <script src="https://gist.github.com/x0axz/8eaf7656f854677cb501679867cbd281.js"></script>

          <h4>Output:</h4>

          Here is a visual representation illustrating the forward pass of the tanh function. <br /> <br />

          <img src="../static/diagrams/autograd/Example 1 - Forward Pass.png" width="100%" alt="Example 1 - Forward pass diagram" />

          <h3>Example 1: Performing backward propagation and visualizing the updated computation graph</h3>

          Compute gradients for the variable <code class="language-python">o</code> through backward propagation to determine its impact on the hyperbolic tangent function (tanh). <br /> <br />

          <script src="https://gist.github.com/x0axz/0963cde963e5cbc0b9e5269f5e2c43d4.js"></script>

          <h4>Output:</h4>

          Here is a visual representation illustrating the backward propagation process of the tanh function. <br /> <br />

          <img src="../static/diagrams/autograd/Example 1 - Backward Propagation.png" width="100%" alt="Example 1 - Backward propagation diagram" />

          <h3>Example 2: Compute a forward pass on scalar values, perform backward propagation, and visualize the computation graph</h3>

          In this second example, we will execute a forward pass on the tanh function, followed by the computation of gradients through backward propagation, and then visualize the results. The inputs, weights, and biases will be scalar values, identical to those used in Example 1, to ensure precise calculations. Feel free to copy the code and modify these values to observe varying outcomes. <br /> <br />

          <script src="https://gist.github.com/x0axz/c33b66e9bdea6ae73d82a6593b828d3f.js"></script>

          <h4>Output:</h4>

          Here is a visual representation that illustrates both the forward pass and backward propagation processes of the tanh function. Please compare this graph with the one from Example 1 to ensure that the gradient values are the same. <br /> <br />

          <img src="../static/diagrams/autograd/Example 2 - tanh.png" width="100%" alt="Example 2 - tanh diagram" />

          <h3>Example 3: Compute a forward pass and perform backward propagation on the tanh function using PyTorch</h3>

          In this example, we will use the same values for the input, weight, and bias variables, along with the tanh function from the PyTorch library. The purpose is to validate the accuracy and consistency of the calculations carried out by our Autograd class. By comparing the results, we can confirm their similarity. <br /> <br />

          <iframe src="../notebooks/autograd/torch_implement_backpropagation.html" width="100%" height="700px"></iframe>

          <h3>Building a neural network library</h3>

          We will now build a neural network library that consists of three classes: Neuron, Layer, and MLP. The Neuron class defines a single neuron in a multi-layer perceptron (MLP) with randomly assigned weights and bias. It performs computations and applies the hyperbolic tangent activation function. The Layer class represents a layer of neurons and computes outputs based on inputs. The MLP class constructs an MLP with a customizable architecture, allowing inputs to propagate through all layers to produce a final output. Both the Neuron and Layer classes have a parameters method to retrieve their specific parameters. Overall, the code offers the necessary functionality for defining and utilizing MLP models, enabling the propagation of inputs and the retrieval of parameters. <br /> <br />

          Let's proceed with creating one class at a time and meticulously analyze each line of the code to provide an explanation of its purpose and functionality. <br /> <br />

          <script src="https://gist.github.com/x0axz/1908ff99080ede4ac242356cd316420c.js"></script>

          The code imports the <code class="language-python">random</code> module, which is used for generating random numbers. <br /> <br />

          A <code class="language-python">Neuron</code> class is defined, representing a single neuron in a neural network.
              
          <ul>
            <li>
              The <code class="language-python">__init__</code> method initializes the neuron with random weights and a bias. The number of input connections (inputs) to the neuron is specified by nin. The weights are randomly initialized using the <code class="language-python">random.uniform</code> function between -1 and 1, and stored in a list <code class="language-python">self.w</code>. The bias is also randomly initialized and stored in <code class="language-python">self.b</code>. <br /> <br />
            </li>

            <li>
              The <code class="language-python">__call__</code> method implements the behavior of the neuron. Given an input <code class="language-python">x</code>, it calculates the weighted sum of inputs multiplied by weights, adds the bias, applies the hyperbolic tangent function to the result (activation), and returns the output of the neuron. <br /> <br />
            </li>

            <li>
              The <code class="language-python">parameters</code> method returns the parameters of the neuron, which include the weights <code class="language-python">(self.w)</code> and the bias <code class="language-python">(self.b)</code>. <br />
            </li>
          </ul>

          In summary, the <code class="language-python">Neuron</code> class represents a single neuron in a neural network. It has methods for initialization, computing the output of the neuron, and retrieving the neuron's parameters. The neuron's weights and bias are randomly initialized within a specified range. The neuron computes its output by calculating the weighted sum of inputs, adding the bias, and applying the hyperbolic tangent function to the result. The neuron's parameters include the weights and bias. <br /> <br />

          <script src="https://gist.github.com/x0axz/cbb27a8896b7329d71f85e9a2ceb2b94.js"></script>
          
          A <code class="language-python">Layer</code> class is defined, representing a layer of neurons in a neural network.

          <ul>
            <li>
              The <code class="language-python">__init__</code> method initializes a layer with a specified number of input neurons (<code class="language-python">nin</code>) and output neurons (<code class="language-python">nout</code>). It creates a list of <code class="language-python">nout</code> neurons by using a list comprehension. Each neuron in the list is created by calling the <code class="language-python">Neuron</code> class with nin as the number of input connections. <br /> <br />
            </li>

            <li>
              The <code class="language-python">__call__</code> method computes the output of each neuron in the layer given an input <code class="language-python">x</code>. It iterates over the neurons in the layer and calls each neuron with <code class="language-python">x</code>, collecting the outputs in a list named <code class="language-python">outs</code>. If there is only one output neuron, it returns the output directly; otherwise, it returns the list of outputs. <br /> <br />
            </li>

            <li>
              The <code class="language-python">parameters</code> method returns the parameters of all neurons in the layer. It achieves this by iterating over the neurons in the layer and calling the <code class="language-python">parameters</code> method of each neuron. The parameters of each neuron are then flattened into a single list using a list comprehension and returned. <br />
            </li>
          </ul>

          In summary, the <code class="language-python">Layer</code> class represents a layer of neurons in a neural network. The layer is initialized with a specified number of input and output neurons. The layer contains a list of neurons, each created with the specified number of input connections. The layer's <code class="language-python">__call__</code> method computes the output of each neuron in the layer given an input. The outputs are collected in a list, and if there is only one output neuron, it is returned directly; otherwise, the list of outputs is returned. The layer's <code class="language-python">parameters</code> method returns the parameters of all neurons in the layer by iterating over the neurons, calling their <code class="language-python">parameters</code> method, and flattening the parameters into a single list. <br /> <br />

          <script src="https://gist.github.com/x0axz/d557428791f44ba7900ebfcf5da0d51d.js"></script>

          An <code class="language-python">MLP</code> (Multi-Layer Perceptron) class is defined, representing a multi-layer neural network.
          
          <ul>
            <li>
              The <code class="language-python">__init__</code> method initializes an MLP with the specified number of input neurons (<code class="language-python">nin</code>) and a list of output neurons for each layer (<code class="language-python">nouts</code>). It creates a list <code class="language-python">sz</code> that contains the sizes of all layers, starting with the input layer size (<code class="language-python">nin</code>) followed by the output sizes of each layer (<code class="language-python">nouts</code>). Each layer in the MLP is created by using a list comprehension and calling the <code class="language-python">Layer</code> class. The size of each layer is determined by <code class="language-python">sz[i]</code> as the number of input neurons and <code class="language-python">sz[i+1]</code> as the number of output neurons. <br /> <br />
            </li>

            <li>
              The <code class="language-python">__call__</code> method performs forward propagation through the MLP. It iterates over the layers in the MLP and sequentially applies each layer to the input <code class="language-python">x</code>. The output of each layer becomes the input for the next layer. Finally, it returns the final output of the MLP. <br /> <br />
            </li>

            <li>
              The <code class="language-python">parameters</code> method returns the parameters of all layers in the MLP. It achieves this by iterating over the layers in the MLP and calling the <code class="language-python">parameters</code> method of each layer. The parameters of each layer are then flattened into a single list using a list comprehension and returned. <br />
            </li>
          </ul>

          In summary, the <code class="language-python">MLP</code> class represents a multi-layer perceptron (MLP), which is a type of neural network. The MLP is initialized with the number of input neurons and a list of output neurons for each layer. The MLP consists of multiple layers, where each layer is represented by the <code class="language-python">Layer</code> class. The <code class="language-python">__call__</code> method performs forward propagation through the MLP by sequentially applying each layer to the input. The final output of the MLP is returned. The <code class="language-python">parameters</code> method returns the parameters of all layers in the MLP by iterating over the layers, calling their <code class="language-python">parameters</code> method, and flattening the parameters into a single list.

          <h4>Perform forward propagation</h4>

          We will now assign input values, build an MLP with a predetermined structure, and then perform forward propagation to calculate and obtain the output. <br /> <br />

          <iframe src="../notebooks/autograd/neural-network-library-forward-pass.html" width="100%" height="250px"></iframe> <br /> <br />

          The code sets the input values <code class="language-python">x</code> to <code class="language-python">[2.0, 3.0, -1.0]</code>. <br /> <br />

          An <code class="language-python">MLP</code> object named <code class="language-python">n</code> is created. It is initialized with 3 input neurons and a list <code class="language-python">[4, 4, 1]</code> representing the number of neurons in each hidden layer and the output layer. Therefore, the MLP has 3 hidden layers, each with 4 neurons, and 1 output neuron. <br /> <br />

          The <code class="language-python">__call__</code> method of the <code class="language-python">MLP</code> object <code class="language-python">n</code> is called with the input values <code class="language-python">x</code>. This triggers the forward propagation of the input through all layers of the MLP.

          <ul>
            <li>
              The <code class="language-python">__call__</code> method sequentially applies each layer in the MLP to the input. It iterates over the layers stored in the <code class="language-python">self.layers</code> attribute of the MLP and applies each layer to the input. The output of each layer becomes the input for the next layer. <br />
            </li>
          </ul>

          The final output of the MLP is returned. <br /> <br />

          In summary, this sets the input values for the MLP, creates an MLP object with a specific architecture (3 input neurons, 3 hidden layers with 4 neurons each, and 1 output neuron), and performs forward propagation by calling the MLP object with the input values. The output of the MLP is then returned.

          <h3>Training a neural network</h3>

          Moving forward, our next steps involve initiating the training process of a neural network. We will commence by creating a small dataset and subsequently proceed to train the MLP model with the goal of minimizing the loss and improving its prediction capabilities. Lastly, we will present the list of predicted outputs.

          <h4>Create dataset</h4>

          Now, our next step is to create a small dataset. <br /> <br />

          <iframe src="../notebooks/autograd/neural-network-dataset.html" width="100%" height="350px"></iframe> <br /> <br />

          The code sets the input values for the training examples in a list named <code class="language-python">xs</code>. Each training example is represented as a list of input values. In this case, there are four training examples, each with three input values. <br /> <br />

          The code sets the desired target values for the training examples in a list named <code class="language-python">ys</code>. Each target value corresponds to a training example in <code class="language-python">xs</code>. In this case, there are four target values. <br /> <br />

          The code provides a comment indicating the correspondence between the training examples and their respective target values. For example, <code class="language-python">xs[0]</code> corresponds to <code class="language-python">ys[0]</code>, <code class="language-python">xs[1]</code> corresponds to <code class="language-python">ys[1]</code>, and so on. <br /> <br />

          In summary, this sets the input values for a set of training examples in the list <code class="language-python">xs</code> and the desired target values in the list <code class="language-python">ys</code>. The correspondence between the training examples and their target values is indicated through the comment. These data are typically used for training a machine learning model, where the model learns to map the input values in <code class="language-python">xs</code> to the corresponding target values in <code class="language-python">ys</code>.

          <h4>Train the MLP model</h4>

          Now start the training process of the MLP model with the objective of minimizing the loss and enhancing its predictive capabilities. <br /> <br />

          Continue iterating until the loss is minimized and the predictions reach the desired level of improvement. <br /> <br />

          <iframe src="../notebooks/autograd/neural-network-loss-function.html" width="100%" height="500px"></iframe> <br /> <br />

          The code initiates a loop that runs for 20 iterations using the <code class="language-python">range(20)</code> function. <br /> <br />

          Within each iteration: <br />

          <ul>
            <li>
              The forward pass is performed by applying the MLP object <code class="language-python">n</code> to each input in the <code class="language-python">xs</code> list using a list comprehension. This generates a list of predicted outputs <code class="language-python">ypred</code> corresponding to each input in <code class="language-python">xs</code>. <br /> <br />
            </li>

            <li>
              The loss is calculated by summing the squared differences between the predicted outputs <code class="language-python">yout</code> and the corresponding target values <code class="language-python">ygt</code> using a generator expression and the <code class="language-python">sum()</code> function. <br /> <br />
            </li>

            <li>
              The backward pass is performed to compute the gradients of the loss with respect to the parameters of the MLP. First, the gradients of all parameters in the MLP are set to zero by iterating over the parameters using the n.<code class="language-python">parameters()</code> method and setting <code class="language-python">p.grad = 0.0</code> for each parameter <code class="language-python">p</code>. <br /> <br />
            </li>

            <li>
              The gradients are computed using the <code class="language-python">backward()</code> method of the loss. This method calculates the gradients using automatic differentiation and the chain rule. <br /> <br />
            </li>

            <li>
              The weights of the MLP are updated by iterating over the parameters and performing a gradient descent update. Each weight <code class="language-python">p.data</code> is updated by subtracting <code class="language-python">0.5 * p.grad</code> from its current value. <br /> <br />
            </li>

            <li>
              The iteration number <code class="language-python">k</code> and the value of the loss are printed using the <code class="language-python">print()</code> function. <br />
            </li>
          </ul>

          In summary, this trains the MLP model for 20 iterations using a training loop. In each iteration, a forward pass is performed to compute the predicted outputs, and the loss is calculated by comparing the predicted outputs to the target values. Then, a backward pass is performed to compute the gradients of the loss with respect to the model parameters. The weights of the model are updated using gradient descent. The iteration number and the value of the loss are printed after each iteration. This process aims to train the MLP model to minimize the loss and improve its predictions.

          <h4>Predicted outputs</h4>

          Lastly, the list of predicted outputs will be displayed. <br /> <br />

          <iframe src="../notebooks/autograd/neural-network-learning-result.html" width="100%" height="200px"></iframe> <br /> <br />

          The code generates a list comprehension where each input <code class="language-python">x</code> in the <code class="language-python">xs</code> list is passed through the MLP object <code class="language-python">n</code>. This applies the MLP's forward propagation to each input and produces a list of predicted outputs. <br /> <br />

          In summary, this calculates the predicted outputs for each input in the <code class="language-python">xs</code> list using the MLP object <code class="language-python">n</code>. The <code class="language-python">ypred</code> list contains these predicted outputs.

          <h3>Summary</h3>

          Neural networks are mathematical expressions, specifically multi-layer perceptrons, that process input data using weights and parameters. They employ a loss function to assess prediction accuracy and aim to minimize this loss through manipulation and backpropagation to obtain gradients. By iteratively adjusting parameters using gradient descent, the network's performance improves over time. Neural networks can tackle complex problems and scale up to billions or trillions of parameters. Training them on large datasets, like GPT for language modeling, unveils emergent properties. Despite slight variations, the fundamental principles of neural network training remain consistent, making the process broadly applicable.

          <h3>Project's code</h3>
          
          <a href="https://github.com/x0axz/autograd">Here</a> is the project's repository, containing the necessary files. Inside the repository, you will discover an <a href="https://github.com/x0axz/autograd/blob/main/autograd/engine.py">engine.py</a> file where I implemented the code for the <code class="language-python">Value</code> class, and a <a href="https://github.com/x0axz/autograd/blob/main/autograd/library.py">library.py</a> file that includes the code for the neural network library. The library.py file imports the <code class="language-python">Value</code> class from the engine.py file. Furthermore, I have included a Jupyter <a href="https://github.com/x0axz/autograd/blob/main/notebook/Autograd_Engine_&_NN_Library.ipynb">notebook</a> file, which you can export to an editor and experiment with.

          <h3>References and further reading</h3>

          <ul>
            <li>
              3Blue1Brown's <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">video series</a> on basics of neural networks and backpropagation. <br />
            </li>

            <li>
              Alammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a> <br />
            </li>

            <li>
              Vaibhav Kumar's <a href="https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95">blog post</a> on PyTorch Autograd. <br />
            </li>

            <li>
              Andrej Karaphy's <a href="https://github.com/karpathy/micrograd">Micrograd</a> and his <a href="https://youtu.be/VMj-3S1tku0">video tutorial</a> about building Micrograd. <br />
            </li>
          </ul>
      </p>
    </div>

    <!-- Single Input Derivative Script -->
    <script>
        // Initial values
        var h = 0.001;
        var x = 3;
        var a = 3.0;
        var b = -4.0;
        var c = 5.0;

        // Differentiation
        function differentiate_single_input_derivative_line_1() {
          var result = (2 * a * x) + b;
          return result;
        }

        // calculate
        function differentiate_single_input_derivative_line_1_plus_h(){
          var result = differentiate_single_input_derivative_line_1() + h;
          return result;
        }

        function f_x_plus_h(){
          var result = ((a * (x + h) ** 2) + (b * (x + h)) + c)
          return result;
        }

        function f_x(){
          var result = ((a * x ** 2 )+ (b * x) + c);
          return result;
        }

        function f_x_plus_h_minus_f_x_divided_h(){
          var result = (f_x_plus_h() - f_x()) / h;
          return result;
        }
    
        // Update result display
        function single_input_derivative_updateResult() {
          document.getElementById("single-input-derivative-result-line-1-final-value").textContent = differentiate_single_input_derivative_line_1();
          document.getElementById("single-input-derivative-result-line-1-final-value-with-h").textContent = differentiate_single_input_derivative_line_1_plus_h();
          document.getElementById("single-input-derivative-result-line-2-final-value-f-x-plus-h").textContent = f_x_plus_h();
          document.getElementById("single-input-derivative-result-line-2-final-value-f-x").textContent = f_x();
          document.getElementById("single-input-derivative-result-line-2-final-value-derivative").textContent = f_x_plus_h_minus_f_x_divided_h();
        }
    
        // Event listeners for slider changes
        d3.select("#single-input-derivative-slider-h").on("input", function () {
          h = +this.value;
          document.getElementById("single-input-derivative-slider-h-value").textContent = h;
          document.getElementById("single-input-derivative-result-line-1-h-value").textContent = h;
          document.getElementById("single-input-derivative-result-line-2-h-value").textContent = h;
          single_input_derivative_updateResult();
        });

        d3.select("#single-input-derivative-slider-x").on("input", function () {
          x = +this.value;
          document.getElementById("single-input-derivative-slider-x-value").textContent = x;
          document.getElementById("single-input-derivative-result-line-1-x-value").textContent = x;
          document.getElementById("single-input-derivative-result-line-2-x-value").textContent = x;
          single_input_derivative_updateResult();
        });

        d3.select("#single-input-derivative-slider-a").on("input", function () {
          a = +this.value;
          document.getElementById("single-input-derivative-slider-a-value").textContent = a;
          document.getElementById("single-input-derivative-result-line-1-a-value").textContent = a;
          document.getElementById("single-input-derivative-result-line-2-a-value-1").textContent = a;
          document.getElementById("single-input-derivative-result-line-2-a-value-2").textContent = a;
          single_input_derivative_updateResult();
        });
    
        d3.select("#single-input-derivative-slider-b").on("input", function () {
          b = +this.value;

          var b_output;
          if (b > 0 || b == 0) {
            b_output = "+" + b;
          } else{
            b_output = b
          }

          document.getElementById("single-input-derivative-slider-b-value").textContent = b;
          document.getElementById("single-input-derivative-result-line-1-b-value").textContent = b_output;
          document.getElementById("single-input-derivative-result-line-2-b-value-1").textContent = b_output;
          document.getElementById("single-input-derivative-result-line-2-b-value-2").textContent = b_output;
          single_input_derivative_updateResult();
        });
    
        d3.select("#single-input-derivative-slider-c").on("input", function () {
          c = +this.value;

          var c_output;
          if (c > 0 || c == 0) {
            c_output = "+" + c;
          } else{
            c_output = c
          }

          document.getElementById("single-input-derivative-slider-c-value").textContent = c;
          document.getElementById("single-input-derivative-result-line-1-c-value").textContent = c_output;
          document.getElementById("single-input-derivative-result-line-2-c-value-1").textContent = c_output;
          document.getElementById("single-input-derivative-result-line-2-c-value-2").textContent = c_output;
          single_input_derivative_updateResult();
        });
    
        // Initial update
        single_input_derivative_updateResult();
    </script>

    <!-- Multiple Input Derivative Script -->
    <script>
      // Initial values
      var h = 0.001;
      var a = 2.0;
      var b = -3.0;
      var c = 10.0;

      // Calculate
      function selectedVariable_value(selectedVariable) {
        var selectedVariable_value;
        switch (selectedVariable) {
          case 'a':
          selectedVariable_value = a;
          break;
          case 'b':
          selectedVariable_value = b;
          break;
          case 'c':
          selectedVariable_value = c;
          break;
          default:
          selectedVariable_value = 0;
          break;
        }
        return selectedVariable_value;
      }

      function selectedVariable_plus_h(selectedVariable){
        var selectedVariable_plus_h = selectedVariable_value(selectedVariable) + h;
        return selectedVariable_plus_h;
      }

      function multiple_input_derivative_result_d1() {
        return (a * b + c);
      }

      function multiple_input_derivative_result_d2(selectedVariable, selectedVariable_plus_h) {
        var result_d2;
        switch (selectedVariable) {
          case 'a':
          result_d2 = selectedVariable_plus_h * b + c;
          break;
          case 'b':
          result_d2 = a * selectedVariable_plus_h + c;
          break;
          case 'c':
          result_d2 = a * b + selectedVariable_plus_h;
          break;
          default:
          result_d2 = 0;
          break;
        }
        return result_d2;
      }

      function multiple_input_derivative_result_slope(selectedVariable, selectedVariable_plus_h) {
        var result_d1 = multiple_input_derivative_result_d1();
        var result_d2 = multiple_input_derivative_result_d2(selectedVariable, selectedVariable_plus_h);
        var result_slope = ((result_d2 - result_d1) / h);
        return result_slope;
      }

      // Differentiation
      function differentiate_multiple_input_derivative(variable) {
        var result;
        switch (variable) {
            case 'a':
            result = b;
            break;
            case 'b':
            result = a;
            break;
            case 'c':
            result = 1.0;
            break;
            default:
            result = 0;
            break;
        }
        return result;
      }

      function differentiate_multiple_input_derivative_plus_h(selectedVariable){
        var derivative_plus_h = differentiate_multiple_input_derivative(selectedVariable) + h;
        return derivative_plus_h;
      }
    
      // Update result display
      function multiple_input_derivative_updateResult() {
        var selectedVariable = document.getElementById("multiple-input-derivative-variable-selector").value;
        document.getElementById("multiple-input-derivative-selected-variable-1").textContent = selectedVariable;
        document.getElementById("multiple-input-derivative-selected-variable-2").textContent = selectedVariable;
        document.getElementById("multiple-input-derivative-result-d1").textContent = multiple_input_derivative_result_d1();
        document.getElementById("multiple-input-derivative-result-selectedVariable-plus-h").textContent = selectedVariable_plus_h(selectedVariable);
        document.getElementById("multiple-input-derivative-result-d2").textContent = multiple_input_derivative_result_d2(selectedVariable, selectedVariable_plus_h(selectedVariable));
        document.getElementById("multiple-input-derivative-result-slope").textContent = multiple_input_derivative_result_slope(selectedVariable, selectedVariable_plus_h(selectedVariable));
        document.getElementById("multiple-input-derivative-result-differentiate").textContent = differentiate_multiple_input_derivative(selectedVariable);
        document.getElementById("multiple-input-derivative-result-differentiate-plus-h").textContent = differentiate_multiple_input_derivative_plus_h(selectedVariable);
      }
    
      // Event listeners for slider changes
      d3.select("#multiple-input-derivative-slider-h").on("input", function () {
        h = +this.value;
        document.getElementById("multiple-input-derivative-slider-h-value").textContent = h;
        document.getElementById("multiple-input-derivative-result-h-value").textContent = h;
        multiple_input_derivative_updateResult();
      });
    
      d3.select("#multiple-input-derivative-slider-a").on("input", function () {
        a = +this.value;
        document.getElementById("multiple-input-derivative-slider-a-value").textContent = a;
        multiple_input_derivative_updateResult();
      });
    
      d3.select("#multiple-input-derivative-slider-b").on("input", function () {
        b = +this.value;
        document.getElementById("multiple-input-derivative-slider-b-value").textContent = b;
        multiple_input_derivative_updateResult();
      });
    
      d3.select("#multiple-input-derivative-slider-c").on("input", function () {
        c = +this.value;
        document.getElementById("multiple-input-derivative-slider-c-value").textContent = c;
        multiple_input_derivative_updateResult();
      });
    
      // Event listener for variable selection change
      d3.select("#multiple-input-derivative-variable-selector").on("change", function () {
        multiple_input_derivative_updateResult();
      });
    
      // Initial update
      multiple_input_derivative_updateResult();
    </script>
</body>

</html>
