{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGrb5mbFqW5V",
        "outputId": "43dfd1e6-1319-416e-fc1a-bbd3daac8410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7071067094802856\n",
            "------------\n",
            "x1 -1.5000003576278687\n",
            "w1 1.000000238418579\n",
            "x2 0.5000001192092896\n",
            "w2 -0.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# create tensors and enable gradient tracking\n",
        "\n",
        "# inputs x1,x2\n",
        "x1 = torch.Tensor([2.0]); x1.requires_grad = True \n",
        "x2 = torch.Tensor([-.0]); x2.requires_grad = True\n",
        "\n",
        "# weights w1, w2\n",
        "w1 = torch.Tensor([-3.0]); w1.requires_grad = True  \n",
        "w2 = torch.Tensor([1.0]); w2.requires_grad = True  \n",
        "\n",
        "# bias of the neurons\n",
        "b = torch.Tensor([6.8813735870195432]); b.requires_grad = True\n",
        "\n",
        "# perform the computation: n = x1*w1 + x2*w2 + b\n",
        "n = x1*w1 + x2*w2 + b\n",
        "\n",
        "# apply the hyperbolic tangent function to n\n",
        "o = torch.tanh(n)\n",
        "\n",
        "print(o.item())\n",
        "\n",
        "# perform backward propagation to compute gradients\n",
        "o.backward()\n",
        "\n",
        "print(\"------------\")\n",
        "print(\"x1\", x1.grad.item())\n",
        "print(\"w1\", w1.grad.item())\n",
        "print(\"x2\", x2.grad.item())\n",
        "print(\"w2\", w2.grad.item())"
      ]
    }
  ]
}